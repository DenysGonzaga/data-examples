{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DynamicPartitionInserts.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzn/bEnyYkYe6saxSmxiWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenysNunes/data-examples/blob/main/spark/advanced/DynamicPartitionInserts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Partition Inserts\n",
        "\n",
        "Example using Dynamic Partition Inserts. <br>\n",
        "This technique is used to overwrite a single partition instead of all data.\n",
        "Read more about [here](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-dynamic-partition-inserts.html)."
      ],
      "metadata": {
        "id": "zR-_l_KMqNyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init spark"
      ],
      "metadata": {
        "id": "KslJXmDbovN4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4zf-u_ukobIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90592921-3860-4ef6-9cca-aab91ca7d933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tree is already the newest version (1.7.0-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "!pip install -q pyspark==3.1.1\n",
        "!sudo apt install tree\n",
        "!rm -rf /tmp/dynpartition/df1/\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName(\"New Session Example\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sql(\"drop table if exists tb_parquet_persons\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving a first dataFrame table"
      ],
      "metadata": {
        "id": "q2bajL8VotYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import Row\n",
        "\n",
        "raw_rows = [\n",
        "        Row(id=1, name='Jonh', partition_id=1),\n",
        "        Row(id=2, name='Maria', partition_id=1),\n",
        "        Row(id=3, name='Ben', partition_id=2)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(raw_rows)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hZL6Mibo36b",
        "outputId": "2b6fd35a-610f-439f-fd04-5dd2a8e49888"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------------+\n",
            "| id| name|partition_id|\n",
            "+---+-----+------------+\n",
            "|  1| Jonh|           1|\n",
            "|  2|Maria|           1|\n",
            "|  3|  Ben|           2|\n",
            "+---+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.saveAsTable(path='/tmp/dynpartition/df1/', name='tb_parquet_persons', partitionBy='partition_id')"
      ],
      "metadata": {
        "id": "zwtp2ELhpD06"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tree /tmp/dynpartition/df1/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnTBH4YWpQ6x",
        "outputId": "578639b7-da64-473b-b855-5ef099c43dbc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/dynpartition/df1/\n",
            "├── partition_id=1\n",
            "│   ├── part-00000-7b84afce-73f5-4a2f-abd4-915b2a733dcf.c000.snappy.parquet\n",
            "│   └── part-00001-7b84afce-73f5-4a2f-abd4-915b2a733dcf.c000.snappy.parquet\n",
            "├── partition_id=2\n",
            "│   └── part-00001-7b84afce-73f5-4a2f-abd4-915b2a733dcf.c000.snappy.parquet\n",
            "└── _SUCCESS\n",
            "\n",
            "2 directories, 4 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving a new df over partition 2"
      ],
      "metadata": {
        "id": "PGpAuF9Zp2hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_rows_2 = [\n",
        "        Row(id=4, name='Oliver', partition_id=2),\n",
        "        Row(id=5, name='Agata', partition_id=2)\n",
        "]\n",
        "\n",
        "df_2 = spark.createDataFrame(raw_rows_2)\n",
        "df_2.registerTempTable(\"tb_parquet_new_persons\")"
      ],
      "metadata": {
        "id": "ttp---VnpiaL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "\n",
        "INSERT OVERWRITE TABLE tb_parquet_persons\n",
        "PARTITION(partition_id = 2)\n",
        "SELECT id, name FROM tb_parquet_new_persons\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyOD5sMTq8pB",
        "outputId": "26390e11-bc07-4b72-e11b-0499e52f17bb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree /tmp/dynpartition/df1/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xux-qEL_rLC6",
        "outputId": "2df42e7b-4788-4476-b2f5-6accf0b46ba6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/dynpartition/df1/\n",
            "├── partition_id=1\n",
            "│   ├── part-00000-7b84afce-73f5-4a2f-abd4-915b2a733dcf.c000.snappy.parquet\n",
            "│   └── part-00001-7b84afce-73f5-4a2f-abd4-915b2a733dcf.c000.snappy.parquet\n",
            "├── partition_id=2\n",
            "│   ├── part-00000-d61131d8-1eea-4355-8a69-833134058261.c000.snappy.parquet\n",
            "│   └── part-00001-d61131d8-1eea-4355-8a69-833134058261.c000.snappy.parquet\n",
            "└── _SUCCESS\n",
            "\n",
            "2 directories, 5 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verifying a new data source\n",
        "\n",
        "Notice that partition 2 was overwritten"
      ],
      "metadata": {
        "id": "la9HljfwsAZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "\n",
        "SELECT * FROM tb_parquet_persons\n",
        "order by id, partition_id\n",
        "\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d54O2x5SrMq7",
        "outputId": "25d41aff-a0b9-4d08-c7f3-a7786bbe7015"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------------+\n",
            "| id|  name|partition_id|\n",
            "+---+------+------------+\n",
            "|  1|  Jonh|           1|\n",
            "|  2| Maria|           1|\n",
            "|  4|Oliver|           2|\n",
            "|  5| Agata|           2|\n",
            "+---+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6yvJUuSXrS5P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}