{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "validation_by_schema",
      "provenance": [],
      "authorship_tag": "ABX9TyMS5wJoSvvhW/5vv/j859bH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenysNunes/data-examples/blob/main/spark/intermediate/validation_by_schema.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9xip8H7MNS3"
      },
      "source": [
        "# Validation by Schema\n",
        "\n",
        "This notebook is a example how to validate a dataframe using a existing schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNenlJ2eFxVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64cd8a8-6085-4fb5-e0e5-cb10d86da6b6"
      },
      "source": [
        "!pip install -q pyspark==3.1.1\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import Row, StringType\n",
        "from pyspark.sql.functions import lit\n",
        "from datetime import date\n",
        "import random\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName(\"New Session Example\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def get_random():\n",
        "  return random.randrange(2000, 5000, 100)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.3 MB 14 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 57.8 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7_kDIsyGBS7"
      },
      "source": [
        "Default database with right schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UUFY7k4F4Wa"
      },
      "source": [
        "raw_rows = [\n",
        "        Row(id=1, name='John', salary=get_random(), hire_date=date(2020, 1, 1)),\n",
        "        Row(id=2, name='Joana', salary=get_random(), hire_date=date(2020, 1, 1)),\n",
        "        Row(id=3, name='Maria', salary=get_random(), hire_date=date(2020, 1, 2)),\n",
        "        Row(id=4, name='Sandra', salary=get_random(), hire_date=date(2020, 1, 2)),\n",
        "        Row(id=5, name='Ben', salary=get_random(), hire_date=date(2020, 1, 3)),\n",
        "        Row(id=6, name='Carl', salary=get_random(), hire_date=date(2020, 1, 3)),\n",
        "        Row(id=7, name='Joseph', salary=get_random(), hire_date=date(2020, 1, 4)),\n",
        "        Row(id=8, name='Oliver', salary=get_random(), hire_date=date(2020, 1, 4))\n",
        "]\n",
        "\n",
        "spark.sql(\"drop table if exists tb_parquet_salaries\")\n",
        "\n",
        "df = spark.createDataFrame(raw_rows)\n",
        "df.write.saveAsTable(path='/tmp/schema-validation/tables/tb_parquet_salaries/', \n",
        "                     name='tb_parquet_salaries',\n",
        "                     mode='overwrite')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxk06YaWGHWl"
      },
      "source": [
        "Generating CSV data with corrupt data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQhy9I7RGP9u"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "output = '/tmp/schema-validation/input/'\n",
        "\n",
        "if not os.path.exists(output):\n",
        "  os.mkdir(output)\n",
        "\n",
        "header = ['id', 'name', 'salary', 'hire_date']\n",
        "data = [\n",
        "    [9, 'Phillip', get_random(), '2020-03-03'],\n",
        "    ['failed schema'],\n",
        "    [11, 'Mark', get_random()],\n",
        "    [12, 'Andersen', get_random(), '2020-03-05']\n",
        "]\n",
        "\n",
        "with open(f'{output}salary.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "\n",
        "    for d in data:\n",
        "       writer.writerow(d)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiFG1SEXIWQp",
        "outputId": "2e665544-0878-4391-e23a-d861f38099f9"
      },
      "source": [
        "!cat /tmp/schema-validation/input/salary.csv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id,name,salary,hire_date\r\n",
            "9,Phillip,2900,2020-03-03\r\n",
            "failed schema\r\n",
            "11,Mark,4100\r\n",
            "12,Andersen,3400,2020-03-05\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7l_BgKKJPIN",
        "outputId": "7b2abdff-6de6-47b1-d299-852e0d587d3a"
      },
      "source": [
        "df_schema = spark.sql(\"select * from tb_parquet_salaries limit 1\")\n",
        "df_schema = df_schema.withColumn(\"bad_record\", lit(None).cast(StringType()))\n",
        "\n",
        "df_source = spark.read.option(\"mode\",\"permissive\") \\\n",
        "                          .option(\"sep\", \",\") \\\n",
        "\t\t\t  .option(\"header\", \"true\") \\\n",
        "\t\t\t  .option(\"columnNameOfCorruptRecord\", \"bad_record\") \\\n",
        "\t\t\t  .csv(output, schema=df_schema.schema) \\\n",
        "        .cache() # must be cached \n",
        "\n",
        "df_source.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+------+----------+-------------+\n",
            "|  id|    name|salary| hire_date|   bad_record|\n",
            "+----+--------+------+----------+-------------+\n",
            "|   9| Phillip|  2900|2020-03-03|         null|\n",
            "|null|    null|  null|      null|failed schema|\n",
            "|  11|    Mark|  4100|      null| 11,Mark,4100|\n",
            "|  12|Andersen|  3400|2020-03-05|         null|\n",
            "+----+--------+------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gm5SBCsMIK1"
      },
      "source": [
        "DF with Failed Lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSE7GZNaMHzV",
        "outputId": "723f2128-e0bc-474c-9dac-2ef1b18798c2"
      },
      "source": [
        "df_source.select(\"bad_record\").where(\"bad_record is not NULL\").show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|   bad_record|\n",
            "+-------------+\n",
            "|failed schema|\n",
            "| 11,Mark,4100|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l14kgFlwMKnN"
      },
      "source": [
        "DF with Correct Lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_izaUvXiLZlI",
        "outputId": "2dca9fdb-8acd-4077-d15a-f22c723f4d45"
      },
      "source": [
        "df_cleared = df_source.select(\"id\", \"name\", \"salary\", \"hire_date\").where(\"bad_record is NULL\")\n",
        "\n",
        "df_cleared.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+------+----------+\n",
            "| id|    name|salary| hire_date|\n",
            "+---+--------+------+----------+\n",
            "|  9| Phillip|  2900|2020-03-03|\n",
            "| 12|Andersen|  3400|2020-03-05|\n",
            "+---+--------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD6a4M18NXho"
      },
      "source": [
        "Saving Correct DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qncpZ2ObM08c"
      },
      "source": [
        "df_cleared.write.saveAsTable(name='tb_parquet_salaries', mode='append')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNWfrqoxNsFX"
      },
      "source": [
        "Selecting..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcl4AieRNfd6",
        "outputId": "62d33d4b-f0ce-4c76-e907-7f4209dea80b"
      },
      "source": [
        "spark.table(\"tb_parquet_salaries\").orderBy(\"id\").show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+------+----------+\n",
            "| id|    name|salary| hire_date|\n",
            "+---+--------+------+----------+\n",
            "|  1|    John|  4700|2020-01-01|\n",
            "|  2|   Joana|  4800|2020-01-01|\n",
            "|  3|   Maria|  2100|2020-01-02|\n",
            "|  4|  Sandra|  2000|2020-01-02|\n",
            "|  5|     Ben|  4000|2020-01-03|\n",
            "|  6|    Carl|  4900|2020-01-03|\n",
            "|  7|  Joseph|  3200|2020-01-04|\n",
            "|  8|  Oliver|  4000|2020-01-04|\n",
            "|  9| Phillip|  2900|2020-03-03|\n",
            "| 12|Andersen|  3400|2020-03-05|\n",
            "+---+--------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbFEZe-tNw9s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}